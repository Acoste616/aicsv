# Fixed Content Processor - Cloud API Support

ğŸš€ **Ulepszona wersja Fixed Content Processor z obsÅ‚ugÄ… cloud APIs**

## ğŸ“‹ Nowe funkcje

âœ… **ObsÅ‚uga wielu providerÃ³w cloud API**
- OpenAI GPT (3.5-turbo, GPT-4)
- Anthropic Claude (Haiku, Sonnet, Opus)
- Google Gemini Pro
- KompatybilnoÅ›Ä‡ wsteczna z lokalnym LLM

âœ… **Zaawansowane funkcje**
- Rate limiting (automatyczny)
- Retry logic z exponential backoff
- Cache odpowiedzi (oszczÄ™dza koszty)
- Konfiguracja przez zmienne Å›rodowiskowe
- ObsÅ‚uga bÅ‚Ä™dÃ³w i fallback

## ğŸ”§ Instalacja

```bash
pip install requests
```

## ğŸ”‘ Konfiguracja kluczy API

### 1. OpenAI
```bash
# UtwÃ³rz konto na https://platform.openai.com/
export OPENAI_API_KEY=your-openai-api-key
```

### 2. Anthropic
```bash
# UtwÃ³rz konto na https://console.anthropic.com/
export ANTHROPIC_API_KEY=your-anthropic-api-key
```

### 3. Google
```bash
# UtwÃ³rz projekt na https://console.cloud.google.com/
# WÅ‚Ä…cz Gemini API
export GOOGLE_API_KEY=your-google-api-key
```

## ğŸ’¡ PrzykÅ‚ady uÅ¼ycia

### Podstawowe uÅ¼ycie

```python
from fixed_content_processor import FixedContentProcessor

# Lokalny LLM (domyÅ›lny)
processor = FixedContentProcessor()

# OpenAI
processor = FixedContentProcessor(
    provider="openai",
    api_key="your-openai-api-key"
)

# Anthropic
processor = FixedContentProcessor(
    provider="anthropic", 
    api_key="your-anthropic-api-key"
)

# Google
processor = FixedContentProcessor(
    provider="google",
    api_key="your-google-api-key"
)
```

### Konfiguracja przez zmienne Å›rodowiskowe

```bash
# Ustaw provider i klucz API
export LLM_PROVIDER=anthropic
export API_KEY=your-anthropic-api-key

# Teraz moÅ¼esz uÅ¼yÄ‡ bez parametrÃ³w
python your_script.py
```

```python
# Automatyczne wykrywanie providera
processor = FixedContentProcessor()  # UÅ¼yje zmiennych Å›rodowiskowych
```

### PrzykÅ‚ad z konkretnym uÅ¼yciem

```python
import os
from fixed_content_processor import FixedContentProcessor

# PrzykÅ‚ad z Anthropic
processor = FixedContentProcessor(
    provider="anthropic",
    api_key=os.getenv("ANTHROPIC_API_KEY"),
    model="claude-3-haiku-20240307"  # Opcjonalnie
)

# PrzetwÃ³rz tweet
result = processor.process_single_item(
    url="https://x.com/example/status/123456789",
    tweet_text="Jak uÅ¼ywaÄ‡ AI w programowaniu",
    extracted_content="SzczegÃ³Å‚owy artykuÅ‚ o AI..."
)

if result:
    print(f"TytuÅ‚: {result['title']}")
    print(f"Opis: {result['short_description']}")
    print(f"Kategoria: {result['category']}")
    print(f"Tagi: {result['tags']}")

processor.close()
```

## ğŸ¯ ObsÅ‚ugiwane modele

### OpenAI
- `gpt-3.5-turbo` (domyÅ›lny)
- `gpt-4`
- `gpt-4-turbo`

### Anthropic
- `claude-3-haiku-20240307` (domyÅ›lny)
- `claude-3-sonnet-20240229`
- `claude-3-opus-20240229`

### Google
- `gemini-pro` (domyÅ›lny)
- `gemini-pro-vision`

## ğŸ“Š Limity i koszty

| Provider | Rate Limit | Koszt (approx) |
|----------|------------|----------------|
| OpenAI   | 60 req/min | $0.001/1K tokens |
| Anthropic| 50 req/min | $0.00025/1K tokens |
| Google   | 60 req/min | $0.00025/1K tokens |
| Local    | Brak       | Darmowe |

## ğŸ› ï¸ Zaawansowane opcje

### Custom rate limiting
```python
from fixed_content_processor import OpenAIProvider

provider = OpenAIProvider(
    api_key="your-key",
    model="gpt-4"
)
provider.rate_limiter.requests_per_minute = 30  # Wolniejszy limit
```

### Retry configuration
```python
# Retry logic jest automatyczny, ale moÅ¼esz dostosowaÄ‡:
# - Exponential backoff z jitter
# - Maksymalnie 3 prÃ³by
# - Automatyczne rate limiting
```

### Cache management
```python
processor = FixedContentProcessor(provider="openai")

# Cache jest automatyczny i zapisany do pliku:
# cache_llm_openai.json
# cache_llm_anthropic.json
# cache_llm_google.json
# cache_llm_local.json
```

## ğŸ”„ Migracja z lokalnego LLM

JeÅ›li masz juÅ¼ dziaÅ‚ajÄ…cy kod z lokalnym LLM:

```python
# Stary kod
processor = FixedContentProcessor()

# Nowy kod - dodaj tylko parametry
processor = FixedContentProcessor(
    provider="openai",
    api_key="your-key"
)

# Wszystko inne pozostaje takie samo!
```

## ğŸ“ Struktura plikÃ³w

```
fixed_content_processor.py    # GÅ‚Ã³wna klasa z cloud API
example_cloud_api.py         # PrzykÅ‚ady uÅ¼ycia
cache_llm_*.json            # Pliki cache dla rÃ³Å¼nych providerÃ³w
```

## ğŸ› Troubleshooting

### BÅ‚Ä…d: "API key is required"
```bash
# Upewnij siÄ™, Å¼e ustawiÅ‚eÅ› klucz API:
export API_KEY=your-api-key
# lub
export OPENAI_API_KEY=your-openai-key
```

### BÅ‚Ä…d: "Rate limit exceeded"
```python
# Rate limiting jest automatyczny, poczekaj chwilÄ™
# Lub uÅ¼yj wolniejszego providera
```

### BÅ‚Ä…d: "JSON parsing failed"
```python
# Retry logic jest automatyczny
# Fallback uÅ¼ywa lokalnego cache'a
```

## ğŸ“ˆ PorÃ³wnanie providerÃ³w

| Feature | OpenAI | Anthropic | Google | Local |
|---------|--------|-----------|--------|-------|
| SzybkoÅ›Ä‡ | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­ |
| JakoÅ›Ä‡ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| Koszt | ğŸ’°ğŸ’°ğŸ’° | ğŸ’°ğŸ’° | ğŸ’°ğŸ’° | ğŸ†“ |
| DostÄ™pnoÅ›Ä‡ | âœ… | âœ… | âœ… | âš ï¸ |

## ğŸš€ Recommended setup

Dla najlepszej kombinacji jakoÅ›ci i kosztu:

```bash
export LLM_PROVIDER=anthropic
export API_KEY=your-anthropic-api-key
```

```python
processor = FixedContentProcessor(
    provider="anthropic",
    api_key=os.getenv("ANTHROPIC_API_KEY"),
    model="claude-3-haiku-20240307"  # Szybki i tani
)
```

## ğŸ“ Support

W przypadku problemÃ³w:
1. SprawdÅº logi (`logging.basicConfig(level=logging.DEBUG)`)
2. SprawdÅº konfiguracjÄ™ zmiennych Å›rodowiskowych
3. Upewnij siÄ™, Å¼e masz aktywne klucze API
4. SprawdÅº limity na swoim koncie

## ğŸ”® Roadmap

- [ ] ObsÅ‚uga Azure OpenAI
- [ ] ObsÅ‚uga Cohere
- [ ] ObsÅ‚uga obrazÃ³w (multimodal)
- [ ] Batch processing
- [ ] Monitoring i metryki