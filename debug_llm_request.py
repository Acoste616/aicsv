#!/usr/bin/env python3
"""
Debug LLM request - sprawdzenie co wysyÅ‚amy do API
"""

import requests
import json

def debug_llm_request():
    """Debuguje Å¼Ä…danie LLM"""
    print("ğŸ” DEBUG LLM REQUEST")
    print("=" * 50)
    
    # Recreate the exact payload from bookmark_processor_fixed
    payload = {
        "model": "mistralai/mistral-7b-instruct-v0.3",
        "messages": [
            {
                "role": "system", 
                "content": "JesteÅ› ekspertem analizy treÅ›ci. Zawsze zwracasz WYÅÄ„CZNIE poprawny JSON bez dodatkowego tekstu, komentarzy czy formatowania markdown."
            },
            {
                "role": "user",
                "content": "Test prompt: zwrÃ³Ä‡ JSON z title i summary dla tekstu 'test content'"
            }
        ],
        "temperature": 0.2,
        "max_tokens": 750,
        "stream": False
    }
    
    print("ğŸ“¤ Payload:")
    print(json.dumps(payload, indent=2, ensure_ascii=False))
    
    try:
        print("\nğŸš€ WysyÅ‚am Å¼Ä…danie...")
        response = requests.post(
            "http://localhost:1234/v1/chat/completions",
            json=payload,
            timeout=30
        )
        
        print(f"ğŸ“Š Status: {response.status_code}")
        print(f"ğŸ“ Headers: {dict(response.headers)}")
        
        if response.status_code != 200:
            print(f"âŒ Error response: {response.text}")
        else:
            data = response.json()
            print("âœ… SUCCESS!")
            print(f"ğŸ“„ Response: {json.dumps(data, indent=2, ensure_ascii=False)}")
            
    except Exception as e:
        print(f"ğŸ’¥ Exception: {e}")

if __name__ == "__main__":
    debug_llm_request() 