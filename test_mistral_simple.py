#!/usr/bin/env python3
"""
Prosty test porÃ³wnawczy Mistral vs poprzedni model
"""

import requests
import json
import time

def test_llm_response(model_name, prompt, temperature=0.3):
    """Test pojedynczej odpowiedzi LLM"""
    print(f"\n=== TEST: {model_name} (temp={temperature}) ===")
    
    try:
        start_time = time.time()
        response = requests.post(
            "http://localhost:1234/v1/chat/completions",
            json={
                "model": model_name,
                "messages": [
                    {
                        "role": "system",
                        "content": "JesteÅ› ekspertem analizy treÅ›ci. Zawsze zwracasz WYÅÄ„CZNIE poprawny JSON bez dodatkowego tekstu."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                "temperature": temperature,
                "max_tokens": 500,
                "stream": False
            },
            timeout=30
        )
        
        response_time = time.time() - start_time
        
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            print(f"âœ… OdpowiedÅº w {response_time:.1f}s ({len(content)} znakÃ³w)")
            
            # PrÃ³ba parsowania JSON
            try:
                json_data = json.loads(content)
                print("âœ… JSON poprawny!")
                print(f"ğŸ“‹ Pola: {list(json_data.keys())}")
                return True, json_data, response_time
            except json.JSONDecodeError as e:
                print(f"âŒ BÅ‚Ä…d JSON: {e}")
                print(f"ğŸ“ Fragment odpowiedzi: {content[:200]}...")
                return False, content, response_time
        else:
            print(f"âŒ HTTP Error: {response.status_code}")
            return False, None, 0
            
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d poÅ‚Ä…czenia: {e}")
        return False, None, 0

def main():
    print("PORÃ“WNANIE MISTRAL vs LLAMA")
    print("=" * 50)
    
    # Prosty prompt testowy
    test_prompt = """Przeanalizuj ten tweet i zwrÃ³Ä‡ JSON:

Tweet: "WÅ‚aÅ›nie odkryÅ‚em niesamowity przewodnik po systemach RAG z @LangChain! PodejÅ›cie krok po kroku do strategii chunking to zÅ‚oto ğŸ”¥ https://example.com #AI #RAG"

ZwrÃ³Ä‡ JSON z polami:
- title: krÃ³tki tytuÅ‚ (max 8 sÅ‚Ã³w)
- summary: opis w 2 zdaniach  
- keywords: lista 5 sÅ‚Ã³w kluczowych
- category: jedna z ["Technologia", "Biznes", "Nauka", "Inne"]
- sentiment: ["Pozytywny", "Neutralny", "Negatywny"]

Odpowiedz TYLKO JSON, bez dodatkowego tekstu."""

    models_to_test = [
        "mistralai/mistral-7b-instruct-v0.3",
        "meta-llama-3.1-8b-instruct-hf"  # dla porÃ³wnania jeÅ›li jest dostÄ™pny
    ]
    
    results = {}
    
    for model in models_to_test:
        print(f"\nğŸ¤– Testowanie modelu: {model}")
        
        # Test z rÃ³Å¼nymi temperaturami
        for temp in [0.2, 0.3, 0.5]:
            success, data, response_time = test_llm_response(model, test_prompt, temp)
            
            if model not in results:
                results[model] = []
            
            results[model].append({
                'temperature': temp,
                'success': success,
                'response_time': response_time,
                'data': data if success else None
            })
            
            time.sleep(1)  # KrÃ³tka przerwa
    
    # Podsumowanie
    print("\n" + "=" * 60)
    print("ğŸ“Š PODSUMOWANIE WYNIKÃ“W")
    print("=" * 60)
    
    for model, tests in results.items():
        model_name = model.split('/')[-1] if '/' in model else model
        successful_tests = sum(1 for t in tests if t['success'])
        avg_time = sum(t['response_time'] for t in tests if t['success']) / max(successful_tests, 1)
        
        print(f"\nğŸ¤– {model_name}:")
        print(f"   âœ… Sukces: {successful_tests}/{len(tests)} ({successful_tests/len(tests)*100:.0f}%)")
        print(f"   â±ï¸  Åšredni czas: {avg_time:.1f}s")
        
        if successful_tests > 0:
            # PokaÅ¼ przykÅ‚ad najlepszej odpowiedzi
            best_test = min([t for t in tests if t['success']], key=lambda x: x['temperature'])
            if best_test['data']:
                print(f"   ğŸ“‹ PrzykÅ‚ad (temp={best_test['temperature']}):")
                for key, value in best_test['data'].items():
                    print(f"      {key}: {value}")
    
    # Rekomendacja
    print(f"\n{'='*60}")
    print("ğŸ† REKOMENDACJA")
    print("="*60)
    
    mistral_success = sum(1 for t in results.get("mistralai/mistral-7b-instruct-v0.3", []) if t['success'])
    llama_success = sum(1 for t in results.get("meta-llama-3.1-8b-instruct-hf", []) if t['success'])
    
    if mistral_success > llama_success:
        print("ğŸ¥‡ MISTRAL 7B wygrywa! Lepsze generowanie JSON.")
    elif llama_success > mistral_success:
        print("ğŸ¥‡ LLAMA 3.1 8B wygrywa! Lepsze generowanie JSON.")
    else:
        print("ğŸ¤ Remis - oba modele podobnie skuteczne.")
    
    print(f"\nMistral: {mistral_success}/3 testÃ³w")
    print(f"Llama: {llama_success}/3 testÃ³w")

if __name__ == "__main__":
    main() 